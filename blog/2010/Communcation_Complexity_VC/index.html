<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ishaq  Aden-Ali | Some connections between communication complexity and PAC learning</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2010/Communcation_Complexity_VC/">

<!-- Theming-->




    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Some connections between communication complexity and PAC learning",
      "description": "In this blog post I'll highlight a suprising (and beautiful!) connection between one-way communication complexity and PAC learning that was discovered by Kremer, Nisan and Ron. After going over the proof of their result, We'll take a closer look at this connection with the Equality function as an example and show how their result gives a sub-optimal protocol for Equality in the low error regime.  To remedy this, I'll explain how we can tweak their argument to get communication protocols with exponentially better message lengths for all functions when we consider the error of the communication protocol as a parameter.",
      "published": "2010-08-11 00:00:00 -0700",
      "authors": [
        
        {
          "author": "Ishaq Aden-Ali",
          "authorURL": "https://en.wikipedia.org/wiki/Albert_Einstein",
          "affiliations": [
            {
              "name": "UC Berkeley",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/">
       <span class="font-weight-bold">Ishaq</span>   Aden-Ali
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Some connections between communication complexity and PAC learning</h1>
        <p>In this blog post I'll highlight a suprising (and beautiful!) connection between one-way communication complexity and PAC learning that was discovered by Kremer, Nisan and Ron. After going over the proof of their result, We'll take a closer look at this connection with the Equality function as an example and show how their result gives a sub-optimal protocol for Equality in the low error regime.  To remedy this, I'll explain how we can tweak their argument to get communication protocols with exponentially better message lengths for all functions when we consider the error of the communication protocol as a parameter.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p>In the 90s Kremer, Nisan and Ron proved that (a certain restriction of) the one-way distributional complexity of a boolean function is equivalent to the VC dimension of a related class of functions! 
This beautiful result alligns well with the high-level intution that learning and compression are related.
The first goal of this blog post is to formally state this equivalence and go over one direction of the proof.</p>

<p>Unfortunately, this equivalence only holds when we consider the failure probability of the communcation scheme as a constant (think \(\frac{1}{3}\)).
We’ll take a look at the Equality function (EQ) as the simplest function demonstrating this gap.
To remedy this, we’ll cook up a protocol with a message length that has a much better dependence on the error parameter.
The protocol achieves the “right” message length for EQ and we’ll build it using some simple ideas from learning theory.
<!-- Finally, with this connection to active learning in mind, we will take a look at the Greater-Than (GT) function in the low error regime and show that the seperation first observerved by Kremer, Nisan and Ron between their restricted version of distributional complexity and the usual distributional complexity can be made much better than what their original result implies. -->
But before getting into all of that, we need to agree on some defintions.</p>

<h2 id="some-defintions">Some defintions</h2>

<p>We are interested in the two-party model of communication where there are two players (shocker) that we will name Alice and Bob.
Alice and Bob both recieve a private input and want to jointly compute a function of their inputs with as little communication as possible. 
More formally, Alice has an input \(y\) (that Bob doesn’t know) that lives in some set \(\mathcal{Y}\), and Bob has some input \(x\) (that Alice doesn’t know) that lives in some set \(\mathcal{X}\).
Their goal will be to jointly compute a boolean function \(f: \mathcal{X} \times \mathcal{Y} \to \{0,1\}\) on their inputs \(x\) and \(y\) while communicating as few bits to one another as possible.
We will assume that the their inputs \(x\) and \(y\) are sampled from a distribution \(\mu\) that is known to both players, and we will be intersted in the special case where \(x\) and \(y\) are sampled from a product distribution \(\mu^{\otimes}\) (\(x\) and \(y\) are independent).</p>

<p>We’ll denote by \(\Pi(x,y)\) a (possibly randomized) communication protocol that Alice and Bob use to compute \(f(x,y)\).
Today we will only be concerend with one-way communcation protocols where Alice sends a message (a sequence of bits) to Bob which he will then use to figure out \(f(x,y)\). Intuitively, we need this message to encode some information about \(y\) so that Bob can accurately determine \(f(x,y)\).</p>

<p>For a distribution \(\mu\) over \(\mathcal{X} \times \mathcal{Y}\) and a function \(f\), let \(D_{\epsilon}^{\to}(\mu, f)\) be the one-way distributional complexity with error \(\epsilon\), which is the minimum number of bits a deterministic protocol \(\Pi\) needs to correctly compute \(f(x,y)\) with probability at least \(1-\epsilon\).
Here the probability is over the randomness of the inputs \((x, y) \sim \mu\).
For a function \(f\) and any \(x \in \mathcal{X}, y \in \mathcal{Y}\), let \(R_{\epsilon}^{\to, \text{pub}}(f)\) be the one-way randomized communication complexity with error \(\epsilon\), which is the minimum number of bits a randomzied protocol \(\Pi\) (using public randomness accessible to both Alice and Bob) needs to correctly compute \(f(x,y)\) with probability at least \(1-\epsilon\).
Here the probability is over the public random coins used by the protocol \(\Pi\).</p>

<p>A beautiful result of Yao is the following:</p>

\[R_{\epsilon}^{\to, \text{pub}}(f) = \max_{\mu} D_{\epsilon}^{\to}(\mu, f).\]

<p>That is, the optimal message size in the case where Alice and Bob have worst-case inputs and want to use a randomized strategy to compute \(f(x,y)\) is equal to the optimal message size in the case where Alice and Bob have an average-case input (albeit with the hardest choice of distribution \(\mu\)) and use a deterministic strategy to compute \(f(x,y)\).</p>

<p>Kremer, Nisan and Ron considered a restrtiction of the usual distribution complexity.
In their work, they consider input distribution is given by a product distribution \(\mu^{\otimes} = \mu_x \times \mu_y\).
They defined and studied the quantity \(R_{\epsilon}^{\to,\otimes} \triangleq \max_{\mu^{\otimes}} D_{\epsilon}^{\to}(\mu^{\otimes}, f)\).
I’ll call this the restricted maximum one-way distributional complexity since I can’t come up with a better term.
While the notation on the left hand side is suggestive of a connection to randomized communication complexity, it does not seem like there is an obvious (at least to me) interpretation.</p>

<p>For a boolean function \(f(x,y) : \mathcal{X} \times \mathcal{Y} \to \{0,1\}\) we define its restriction to a fixed \(y \in \mathcal{Y}\) as the function \(f_y(x): \mathcal{X} \to \{0,1\}\) that maps \(x \in \mathcal{X}\) to \(f(x,y)\). 
Finally, define the set of function \(F_\mathcal{Y} \triangleq \{ f_y(x) \mid y \in \mathcal{Y} \}\).</p>

<h2 id="the-connection-to-pac-learning">The connection to PAC learning</h2>

<p>Kremer et al. proved that the restricted maximum one-way distributional complexity of a boolean function \(f\) is <em>equivalent</em> (up to constants) to the <a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension" target="\_blank">VC dimension</a> (which is a measure of complexity or expressiveness of a set of boolean functions) of the set of functions \(F_\mathcal{Y}\):</p>

\[R_{\frac{1}{3}}^{\to, \otimes}(f) = \Theta(VC(F_\mathcal{Y})).\]

<p>The choice of \(\epsilon = \frac{1}{3}\) is unimportant and we could have chosen any other constant smaller than $\frac{1}{2}$. 
This an amazing equivalence and we’ll now go over the proof of the direction \(R_{\frac{1}{3}}^{\to, \text{pub}}(f) \lesssim VC(F_\mathcal{Y})\).</p>

<p>To prove the above direction, we need to come up with a deterministic communication protocol \(\Pi\) that uses about \(VC(F_\mathcal{Y})\) bits.
I’ll now describe the protocol Kremer et al. came up with.
Recall that Alice and Bob both know the distribution \(\mu^{\otimes} = \mu_x \times \mu_y\). 
So, before Alice and Bob recieve their inputs, they can agree on an i.i.d. random sample \(S = \{X_1, \dots, X_n\}\) sampled from the distribution \(\mu_x\). (Recall that this is the distribution over Bob’s input.)
Once Alice and Bob recieve their inputs \(Y\) and \(X\) respectively, Alice knows the restricted function \(f_Y\) and her goal will be to tell Bob the identity of this function.
Her strategy will be to send over the labels that the function \(f_Y\) induces on the sample \(S\). 
That is, she will send over the sequence of bits \(f_Y(X_1), \dots, f_Y(X_n)\). 
Bob will then pick any function \(f_S \in F_Y\) that is consistent with the labels Alice sent over.
Clearly she only communicated \(n\) bits.
Intuitively, the hope is that if we pick enough examples \(S\) that Alice can label for Bob with \(f_Y\), then the function \(f_S\) that Bob decides on that is consistent with Alice’s function on \(S\) will be “good enough.”
What remains is to figure out how large the number of examples \(n\) needs to be to guarantee (with high enough probability) that the function Bob picks is a “good enough” approximation for \(f_Y\) (and to define what we mean by good enough).</p>

<p>A fundamental result in statistical learning theory asserts that for <em>any</em> choice of distribution \(\mu_x\) over \(\mathcal{X}\) and <em>any</em> labeling function \(f: \mathcal{X} \to \{0,1\}\) from a set of functions \(F\), if we recieve \(n \geq m(\epsilon, \delta)\) i.i.d. samples \(S = \{X_1, \dots, X_n\}\) from the distribution \(\mu_x\), and binary labels \(\{f(X_1), \dots, f(X_n)\}\) labelled with the function \(f\), then any function \(\hat{f} \in F\) that labels all \(n\) samples correctly satisfies, with probability at least \(1-\delta\) over the randomness of the sample \(S\),</p>

\[\begin{eqnarray} 
\mathbb{P}_{X \sim \mu_x}[f(X) \not= \hat{f}(X)] \leq \epsilon,
\end{eqnarray}\]

<p>where</p>

\[m(\epsilon, \delta) \lesssim \frac{VC(F)\cdot\log(1/\epsilon) + \log(1/\delta)}{\epsilon}.\]

<p>In otherwords, when \(n\) is large enough, the learned function \(\hat{f}\) satifies</p>

\[\mathbb{P}_{S}[\mathbb{P}_{X \sim \mu_x}[f(X) \not= \hat{f}_{S}(X)] &gt; \epsilon] &lt; \delta.\]

<p>In the context of our communication protocol, Alice determined the labelling function \(f_Y\), and since her and Bob agreed before hand on the sample \(S\), all she has to do is communicate the labels to Bob. 
Since Bob’s strategy is to output any function \(f_{S} \in F_{\mathcal{Y}}\) that is consistent with the labels on the points \(S\) (and there is of course at least one since Alice labelled the points with \(f_Y \in F_{\mathcal{Y}}\)), the above fundamental result from learning theory guarantees us that, for any fixed choice of \(y \in \mathcal{Y}\),</p>

\[\mathbb{P}_{\Pi}[\mathbb{P}_{X \sim \mu_x}[f_{y}(X) \not= f_{S}(X)] &gt; \epsilon] = \mathbb{P}_{S}[\mathbb{P}_{X \sim \mu_x}[f_{y}(X) \not= f_{S}(X)] &gt; \epsilon] &lt; \delta.\]

<p>Since the above inequality holds for <em>any</em> choice of \(y \in \mathcal{Y}\), it must also holds for a random \(Y \sim \mu_y\), so we can conclude that</p>

\[\mathbb{P}_{\Pi}[\mathbb{P}_{(X,Y) \sim \mu^{\otimes}}[f_{Y}(X) \not= f_{S}(X)] &gt; \epsilon] &lt; \delta.\]

<p>What have we achieved? We have shown that the randomized protocol \(\Pi\) that Alice and Bob have agreed on guarantees that with probability at least \(1-\delta\) (over the randomness of \(\Pi\)), the function \(f_S\) that Bob decides on after recieving Alice’s message will satisfy \(\mathbb{P}_{(X,Y)\sim \mu^{\otimes}}[f_{Y}(X) \not= f_{S}(X)] &lt; \epsilon\) whenever the number of 
examples satisfies</p>

\[n \gtrsim \frac{VC(F_{\mathcal{Y}})\cdot\log(1/\epsilon) + \log(1/\delta)}{\epsilon}.\]

<p>To make this bound a little more useful for us, we can define the event</p>

\[A(S) = \Big\{\mathbb{P}_{(X,Y) \sim \mu^{\otimes}}[f_{Y}(X) \not= f_{S}(X)] \leq \epsilon \Big\}\]

<p>and do the following simple calculation:</p>

\[\begin{aligned} 
\mathbb{E}_{\Pi}[\mathbb{P}_{(X,Y) \sim \mu^{\otimes}}[f_{Y}(X) \not= f_{S}(X)]] = &amp; \ \mathbb{E}_{\Pi}[\mathbb{P}_{(X,Y) \sim \mu^{\otimes}}[f_{Y}(X) \not= f_{S}(X)] \mid A(S)] \cdot \mathbb{P}_{\Pi}[A(S)]  + \\
\quad &amp; \ \mathbb{E}_{\Pi}[\mathbb{P}_{(X,Y) \sim \mu^{\otimes}}[f_{Y}(X) \not= f_{S}(X)] \mid  \neg A(S)] \cdot \mathbb{P}_{\Pi}[\neg A(S)]\\
\leq &amp; \ \epsilon\cdot 1 + 1 \cdot \delta \\
= &amp; \ \epsilon + \delta.
\end{aligned}\]

<p>Because the only randomness in \(\Pi\) comes from \(S\) and the <em>average</em> error of the protocol \(\Pi\) is at most \(\epsilon + \delta\), there must be a <em>fixed</em> choice of \(S\)  —and hence a deterministic \(\Pi\)— that achieves error at most \(\epsilon + \delta\).
So, there exists a deterministic protocol \(\Pi\) that satisfies \(\mathbb{P}_{(X,Y) \sim \mu^{\otimes}}[f_{Y}(X) \not= f_{S}(X)] \leq \epsilon + \delta\), and so if Bob decides on \(f_S(X)\) as the answer his probability of error is at most \(\epsilon + \delta\).</p>

<p>The argument works for any choice of \(\mu^{\otimes}\), so for the constant error regime setting \(\epsilon = \delta = 1/6\) implies</p>

\[R_{\frac{1}{3}}^{\to, \otimes} = \max_{\mu^{\otimes}} D_{\frac{1}{3}}^{\to}(\mu^{\otimes},f) = O\left(VC(F_{\mathcal{Y}})\right),\]

<p>proving the result.</p>

<h2 id="a-better-upperbound-for-small-error">A better upperbound for small error</h2>

<p>When we want the error of the protocol \(\epsilon\) to be very small, the really nice protocol that Kremer et al. came up with gives us</p>

\[R_{\epsilon}^{\to, \otimes} = \max_{\mu^{\otimes}} D_{\epsilon}^{\to}(\mu^{\otimes},f) = O\left(\frac{VC(F_{\mathcal{Y}})\cdot \log(1/\epsilon)}{\epsilon}\right)\]

<p>and this follows by setting \(\epsilon = \delta\) and dividing by 2.
Unfortunately, this bound is linear in \(\epsilon^{-1}\), and this is provably not tight for many simple functions like Equality (EQ).
To see this, let’s calculate what communication complexity the Kremer et al.s bound gives us. 
The equality function \(EQ(x,y): \mathcal{X} \times \mathcal{X} \to \{0,1\}\) is the boolean function that returns \(1\) if and only if \(x = y\).
It is not hard to see that \(F_\mathcal{X} \triangleq \{ EQ_{y}(x) \triangleq EQ(x,y) \mid y \in \mathcal{X} \}\) is the set of indicator functions on the domain \(\mathcal{X}\). 
It is pretty easy to prove that the VC dimension of indicator functions is \(1\), so the bound we have tells us that</p>

\[R_{\epsilon}^{\to, \otimes}(EQ) = O\left(\frac{\log(1/\epsilon)}{\epsilon}\right).\]

<p>Why is this sub-optimal? Notice that</p>

\[R_{\epsilon}^{\to, \otimes}(f) = \max_{\mu^{\otimes}} D_{\epsilon}^{\to}(\mu^{\otimes},f) \leq \max_{\mu} D_{\epsilon}^{\to}(\mu,f) = R_{\epsilon}^{\to, \text{pub}}(f),\]

<p>so an upper bound on \(R_{\epsilon}^{\to, \text{pub}}(f)\) immmediately implies an upper bound on \(R_{\epsilon}^{\to, \otimes}(f)\).
If one took a hypothetical course titled “Communication Complexity 101” they would very quickly learn that \(R_{\epsilon}^{\to, \text{pub}}(EQ) = O(\log(1/\epsilon))\), which is exponentially better than what we can conclude using Kremer et al.’s bound.</p>

<p>This is a bit upsetting since the connection to PAC learning was so nice, and it is tight when \(\epsilon\) is a constant. Can we do better while still insisting on keeping this connection to learning theory? While I don’t know how to <em>characterize</em> the dependency on \(\epsilon\) for every function class (or if that is even possible), it turns out we can improve the upper bound to be \(O(VC(F_{\mathcal{Y}})\cdot \log(1/\epsilon))\).
In other words, we can cook up a communication protocol that is better than Kremer et al’s.
For EQ this recovers the simple bound we can get by appealing to randomized communication complexity of EQ! (Footnote: Although this is not necessarily sharp since we dont know how much easier things are with product distributions in general.)</p>

<h3 id="the-better-protocol">The better protocol</h3>

<p>The protocol is actually very simple and it follows almost immmediately from a simple known fact in learning theory.
An intutive reason for why the protocol we will take a look at is better is because we will really take advantage of the fact that we known \(\mu_x\) apriori.
The result from the previous section did not really take full advantage of this fact, whereas the protocol below will depend heavily on the distribution \(\mu_x\).</p>

<p>Before I explain the protocol, I’ll first need to state that simple result from the learning theory literature I mentioned.
For a fixed distribution \(\mu_x\) and function class \(F\), we can define a distance measure between boolean function on \(\mathcal{X}\) as</p>

\[dist(f,g) \triangleq \mathbb{P}_{X \sim \mu_x}[f(X) \neq g(X)].\]

<p>It is well known fact that for any parameter \(\epsilon\), distribution \(\mu_x\) and set of functions \(F\), there exists a set of functions \(F'\) such that for any \(f \in F\) there is an \(f' \in F'\) such that \(dist(f,f') \leq \epsilon\) and</p>

\[|F'| \leq \left(\frac{41}{\epsilon}\right)^{VC(F)}.\]

<p>Given this fact, the strategy Alice and Bob should use is very simple.
Since they both know \(\mu_x\) and \(F_{\mathcal{Y}}\), for any target error \(\epsilon\) they can agree on a set of function \(F'\) with the above guarantee.
When they recieve their inputs, Alice will communicate to Bob the identity of a function \(f' \in F'\) that satisfies \(dist(f_{Y},f') \leq \epsilon\), and to do this she only needs to send</p>

\[\log\left(\left(\frac{41}{\epsilon}\right)^{VC(F_{\mathcal{Y}})}\right) = O(VC(F_{\mathcal{Y}})\cdot\log(1/\epsilon))\]

<p>bits.
Since this strategy would work for any \(y \in \mathcal{Y}\), it’ll work for a random \(Y \sim \mu_y\) so we can conclude that \(\mathbb{P}_{(X,Y) \sim \mu^{\otimes}}[f_{Y}(X) \neq f'(X)] \leq \epsilon\).
Since Bob’s input \(X\) is sampled from \(\mu_x\) we know that when he decideds on \(f'(X)\) as the answer, the probability that his answer is wrong is at most \(\epsilon\).
So, since the above argument can be applied to any \(\mu^{\otimes}\), we can conclude that</p>

\[R_{\epsilon}^{\to,\otimes}(f) = \max_{\mu^{\otimes}} D_{\epsilon}^{\to}(\mu^{\otimes},f) = O\left(VC(F_{\mathcal{Y}})\cdot \log(1/\epsilon)\right).\]

<h2 id="conclusion">Conclusion</h2>

<p><strong>NOTE:</strong>
Citations, footnotes, and code blocks do not display correctly in the dark mode since distill does not support the dark mode by default.
If you are interested in correctly adding dark mode support for distill, please open <a href="https://github.com/alshedivat/al-folio/discussions">a discussion</a> and let us know.</p>

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2023 Ishaq  Aden-Ali.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib">
  </d-bibliography>

</html>
