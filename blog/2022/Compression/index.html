<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ishaq  Aden-Ali | Distribution Learning Part 2: Compressing and Learning Distributions</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2022/Compression/">

<!-- Theming-->




    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/">
       <span class="font-weight-bold">Ishaq</span>   Aden-Ali
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Distribution Learning Part 2: Compressing and Learning Distributions</h1>
    <p class="post-meta">March 20, 2022</p>
  </header>

  <article class="post-content">
    <p>In this blog post, I’ll explain the core idea in a recent paper by <a href="https://arxiv.org/abs/1710.05209v5" target="blank"> Hassan Ashtiani, Shai Ben-David, Nick Harvey, Chris Liaw, Abbas Mehrabian and Yaniv Plan </a>. This paper introduced a notion of <em>distribution compression</em> and relates this idea to the distribution learning setting (we’ll explain this setting shortly). The take home message of their paper is the following: if a set of distributions can be “compressed”, then it can be “learned”. If you spot any typos or issues, please feel free to send me an email :)</p>

<h3 id="distribution-learning">Distribution learning</h3>

<p>Imagine you have some i.i.d. samples generated from an unknown distribution \(g\), and you want to use these samples to find a distribution \(\hat{g}\) that is “close” to \(g\). If you have no clue as to what \(g\) is, then there is not much you can do. However, if you somehow know that \(g\) belongs to a certain set of distributions<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> — say, \(g\) is a univariate Gaussians with mean \(\mu  \in (-1000,1000)\) and variance \(\sigma^{2} = 1\) — then we have some hope in solving this problem.</p>

<!---
The oldest one I know of is <a href= "https://en.wikipedia.org/wiki/Minimax_estimator" target="blank"> minimax estimation  </a> from the statistics community.

 In the learning theory community, <a href= "https://www.cis.upenn.edu/~mkearns/papers/dist.pdf" target="blank"> {Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire and Linda Sellie} </a> were the first to formalize such a problem, inspired by the probably approximate correct (PAC) learning framework. They named their setting *PAC learning of distributions*. Their specific focus was on the computational complexity of learning
-->
<p>We can formalize the above problem in a number of ways. The setting we will focus on is <em>distribution learning</em>, which was first studied in the learning theory community in the mid 90s, where the focus was on computational efficiency (Cite Kearns). In this setting, an algorithm gets i.i.d. sample access to an unknown distribution \(f\) and outputs a distribution \(\hat{f}\) as an estimate. Furthermore, we require this estimate should be close to the unknown distribution in<a href="https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures" target="blank"> Total Variation (TV) distance </a> (denoted \(\text{d}_{\text{TV}}\)(\(\cdot,\cdot\)) ) with high probability. We can define this problem in the <em>realizable</em> setting as follows:</p>

<hr />
<p><strong>Definition 1</strong>: <em>Let</em> \(\mathcal{F}\) <em>be a set of distributions, and assume we have i.i.d. sample access to some unknown disitribution</em> \(f \in \mathcal{F}\). <em>We say an algorithm is a</em> realizable PAC learner <em>for</em> \(\mathcal{F}\) <em>if for</em> any \(\varepsilon, \delta \in (0,1)\) <em>and</em> \(f\in \mathcal{F}\), <em>if the algorithm is given</em> \(\varepsilon, \delta\) <em>and an i.i.d. sample of size</em> \(m(\varepsilon,\delta)\) <em>from</em> \(f\)<em>as inputs, it outputs a distribution</em> \(\hat{f}\) <em>satisfying</em> \(\text{d}_{\text{TV}}(f,\hat{f}) \leq \varepsilon\), <em>with probability at least</em> \(1-\delta\).</p>

<hr />

<p>There’s a lot going on here, so let’s try and unpack what the definition says. In this setting, we have i.i.d. sample access to an <em>unkown</em> distribution \(f\), and we <em>know</em> that \(f\) belongs to the (possibly infinite) set of distributions \(\mathcal{F}\). A realizable PAC learner will take as input an accuracy parameter \(\varepsilon\), a confidence parameter \(\delta\), and \(m(\varepsilon,\delta)\) i.i.d. samples from \(f\), and will output a distribution \(\hat{f}\). The algorithm guarantees that its output \(\hat{f}\) satisfies \(\text{d}_{\text{TV}}(\hat{f},f) \leq \varepsilon\) with probability at least \(1 - \delta\). In learning theory, \(m(\varepsilon,\delta)\) is called the <em>sample complexity</em> of the learning problem, and this is what we will be most interested in talking about throughout this blogpost.</p>

<p>A fair criticism of the above definition is that we need \(f\) to be in the set \(\mathcal{F}\). If this is doesn’t hold, we unfortunately cannot make any guarantees about the output of the algorithm. In other words, this setting can’t handle “model misspecification”. To over come this, we can borrow ideas from the traditional PAC learning setting and define what it means for an algorithm in the distribution learning setting to be an <em>agnostic</em> PAC learner:</p>

<hr />

<p><strong>Definition 2</strong>: <em>Let</em> \(\mathcal{F}\) <em>be a set of distributions, and assume we have i.i.d. sample access to some unknown disitribution</em> \(g\) <em>such that</em> \(\text{d}_{\text{TV}}(g,\mathcal{F}) = \mathrm{OPT}\)<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>. <em>We say an algorithm is a</em> \(c\)-agnostic PAC learner <em>for</em> \(\mathcal{F}\) <em>if for</em> any \(\varepsilon, \delta \in (0,1)\) <em>and</em> \(g\), <em>if the algorithm is given</em> \(\varepsilon, \delta\) <em>and an i.i.d. sample of size</em> \(m^{c}(\varepsilon,\delta)\) <em>from</em> \(g\) <em>as inputs, it outputs a distribution</em> \(\hat{f}\) <em>satisfying</em> \(\text{d}_{\text{TV}}(g,\hat{f}) \leq c\cdot\mathrm{OPT}+\varepsilon\), <em>with probability at least</em> \(1-\delta\).</p>

<hr />

<p>This definition naturally gives us algorithms that are robust to model misspecification! If our modeling assumption that \(f \in \mathcal{F}\) turned out to be true, great! \(\mathrm{OPT} = 0\) and we get back the guarantee from the realizable setting. If our assumption is false, our algorithm still gives us a guarantee with respect to the best distribution in \(\mathcal{F}\).</p>

<p>A classic result due to Yatracos (and refined by Devroye and Lugosi) gives an algorithm that can agnostically learn w.r.t. any <em>finite</em> set of distributions. We state this result in the same language used above to describe our learning algorithms.</p>

<hr />
<p><strong>Theorem 3</strong>: <em>Let</em> \(\varepsilon, \delta \in (0,1)\) and <em>let</em> \(\mathcal{F} = \{f_{1},f_{2},\dots,f_{M}\}\) <em>be a (finite) set of distributions. Assume we have i.i.d. sample access to some unknown disitribution</em> \(g\) <em>such that</em> \(\text{d}_{\text{TV}}(g,\mathcal{F}) = \mathrm{OPT}\).  <em>There exists a 3-agnostic PAC learner for</em> \(\mathcal{F}\) <em>with sample complexity</em></p>

\[m(\varepsilon,\delta)^{3} = O\left(\frac{\log(M/\delta)}{\varepsilon^{2}}\right).\]

<hr />

<p>So in the case where \(\mathcal{F}\) is finite, we have a 3-agnostic PAC learner! Unfortunately, we get vacuous bounds if we try to use this algorithm for infinite sets of distributions. The good news is that we will see that we can use this algorithm as a subroutine for more powerful methods.</p>

<h3 id="covering-a-first-attempt">Covering: a first attempt</h3>

<p>To simplify things, we will talk about the realizable setting, but all the results we mention will hold in the agnostic setting as well.</p>

<h3 id="compression-schemes">Compression Schemes</h3>

<!---To Do
1. Explain Sample Compression schemes high level: Amal and Bilal: PICTURES
2. Give a formal definition
3. Brief relation to Sample Compression idea from Little Stone and Warmuth

 --->

<h3 id="compression-implies-learning">Compression implies learning</h3>

<!---To Do
1. Explain why an encoder cant exist, but we can brute force the possible signals
2. Theorem: Compression implies learning

 --->

<h3 id="compressing-a-1-dimensional-gaussian">Compressing a 1-dimensional Gaussian</h3>

<!---To Do
  1. Intuitive explination with picturesss~~
  2. Theorem: Compressing in the 1-dimensional case

--->

<h3 id="conclusion">Conclusion</h3>

<!---To Do
1. Compression is cool. Been used to give upper bounds on sample complexity of learning GMMs (w.r.p to TV)
2. We've used compression schemes to give upper bounds on the sample complexity of learning SPNs with tree structures
3. Thanks for reading!

 --->

<h3 id="refrences">Refrences</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>We have to make <em>some</em> modeling assumption. We can be very general (e.g. the set of all univariate distributions with finite 2nd moments) or very specific (e.g. the set of univariate gaussians with mean \(\mu \in \{1,1.01 \}\) and variance \(\sigma^{2} = 1\)). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>The TV distance between a distribution and a set of distributions is defined as \(\text{d}_{\text{TV}}(g,\mathcal{F}) = \inf_{f\in \mathcal{F}} \text{d}_{\text{TV}}(g,f)\). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Ishaq  Aden-Ali.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
