<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ishaq  Aden-Ali</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://raw.githubusercontent.com/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->




    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Ishaq</span>   Aden-Ali
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic2.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I’m a third year CS Ph.D. student at UC Berkeley where I am very fortunate to be co-advised by <a href="https://www.stat.berkeley.edu/~bartlett/" target="\_blank">Peter Bartlett</a> and <a href="https://people.eecs.berkeley.edu/~minilek/" target="\_blank">Jelani Nelson</a>. I recently obtained my M.Sc. from McMaster University where I was very fortunate to be advised by <a href="https://www.cas.mcmaster.ca/ashtiani/" target="\_blank">Hassan Ashtiani</a>. My name is pronounced <a href="https://en.wikipedia.org/wiki/Ishak_(name)" target="\_blank">Is-hak</a>.</p>

<p><strong> Research Interests: </strong></p>

<!-- I am mainly interested in studying problems at the intersection of theoretical computer science, machine learning and statistics. My current research focuses on questions related to [statistical learning theory](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning){:target="\_blank"} and  [distribution learning](https://en.wikipedia.org/wiki/Distribution_learning_theory){:target="\_blank"}. In particular, I'm interested in understanding which problems can (or cannot!) be provably solved by algorithms that are both statistically and computationally efficient. -->

<!--I am also very interested in private data analysis (via [differential privacy](https://en.wikipedia.org/wiki/Differential_privacy){:target="\_blank"}) and mathematical optimization. -->

<!-- 14/03/23 I'm mainly interested in studying problems at the intersection of theoretical computer science, machine learning, and statistics. I'm currently thinking about learning and estimation problems motivated by practical considerations such as data privacy, model misspecification, and memory limitations. In particular, I'm interested in understanding which of these problems can (or cannot!) be provably solved by algorithms that are both statistically and computationally efficient. -->

<p>I’m generally interested in learning theory, statistics, and theoretical computer science.
A particular interest of mine is to understand how we can borrow mathematical tools developed in one of these research areas to solve problems in another.</p>

<!-- Learning Theory: Many classic results in statistical learning theory are based on uniform convergence phenomena that allows one to justify the use of empirical risk minimization procedure. Unfortunately many modern learning settings do not enjoy uniform convergence and cannot enjoy empirical use minimization. Need new algorithmic ideas to do provable learning.
-->

<!-- 
Put your address / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/){:target="\_blank"} and [Academicons](https://jpswalsh.github.io/academicons/){:target="\_blank"}, like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

    </div>

    
      <!-- <div class="news">
  <h2>News</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Nov 9, 2022</th>
          <td>
            
              <a href="https://arxiv.org/abs/2211.03917" target="blank">New paper</a> with Yanjun Han, Jelani Nelson, and Huacheng Yu where we study approximate counting datastructures.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 13, 2021</th>
          <td>
            
              I successfully defended my M.Sc. thesis. I will be joining the EECS department at UC Berkeley as a Ph.D. student.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 7, 2021</th>
          <td>
            
              <a href="https://arxiv.org/abs/2106.02162" target="blank">New paper</a> on privately learning mixtures of axis-aligned Gaussians with fantastic co-authors <a href="https://www.cas.mcmaster.ca/ashtiani/" target="blank">Hassan</a> and <a href="https://cvliaw.github.io/" target="blank">Chris</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 21, 2020</th>
          <td>
            
              <a href="https://arxiv.org/abs/2010.09929" target="blank">Our paper</a> on privately learning high-dimensional Gaussians with <a href="https://www.cas.mcmaster.ca/ashtiani/" target="blank">Hassan</a> and <a href="http://www.gautamkamath.com/" target="blank">Gautam</a> was accepted to <a href="http://algorithmiclearningtheory.org/alt2021/" target="blank">ALT 2021</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 26, 2020</th>
          <td>
            
              I (virtually) presented our <a href="https://arxiv.org/abs/1912.02765" target="blank">paper</a> at AISTATS 2020.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>
 -->
 <!-- Commented out the above to remove news on webiste -->
    

    
      <div class="publications">
  <h2>Selected publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">COLT</abbr>
    
  
  </div>

  <div id="Majority-of-3" class="col-sm-8">
    
      <div class="title">Majority-of-Three: The Simplest Optimal Learner?</div>
      <div class="author">
        
          
            
              
                <em>Ishaq Aden-Ali</em>,
              
            
          
        
          
            
              
                
                  Mikael Møller Høgsgaard,
                
              
            
          
        
          
            
              
                
                  <a href="https://cs.au.dk/~larsen/" target="_blank">Kasper Green Larsen</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://sites.google.com/view/nikitazhivotovskiy/" target="_blank">Nikita Zhivotovskiy</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Learning Theory,</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2403.08831" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Developing an optimal PAC learning algorithm in the realizable setting, where empirical risk minimization (ERM) is suboptimal, was a major open problem in learning theory for decades. The problem was finally resolved by Hanneke a few years ago. Unfortunately, Hanneke’s algorithm is quite complex as it returns the majority vote of many ERM classifiers that are trained on carefully selected subsets of the data. It is thus a natural goal to determine the simplest algorithm that is optimal. In this work we study the arguably simplest algorithm that could be optimal: returning the majority vote of three ERM classifiers. We show that this algorithm achieves the optimal in-expectation bound on its error which is provably unattainable by a single ERM classifier. Furthermore, we prove a near-optimal high-probability bound on this algorithm’s error. We conjecture that a better analysis will prove that this algorithm is in fact optimal in the high-probability regime.
</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">FOCS</abbr>
    
  
  </div>

  <div id="One-Inclusion-Graph-Optimal" class="col-sm-8">
    
      <div class="title">Optimal PAC Bounds Without Uniform Convergence</div>
      <div class="author">
        
          
            
              
                <em>Ishaq Aden-Ali</em>,
              
            
          
        
          
            
              
                
                  <a href="https://yeshwanth94.github.io/" target="_blank">Yeshwanth Cherapanamjeri</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://ashettyv.github.io/" target="_blank">Abhishek Shetty</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://sites.google.com/view/nikitazhivotovskiy/" target="_blank">Nikita Zhivotovskiy</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Symposium on Foundations of Computer Science,</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2304.09167" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In statistical learning theory, determining the sample complexity of realizable binary classification for VC classes was a long-standing open problem. The results of Simon and Hanneke established sharp upper bounds in this setting. However, the reliance of their argument on the uniform convergence principle limits its applicability to more general learning settings such as multiclass classification. In this paper, we address this issue by providing optimal high probability risk bounds through a framework that surpasses the limitations of uniform convergence arguments.
Our framework converts the leave-one-out error of permutation invariant predictors into high probability risk bounds. As an application, by adapting the one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth, we propose an algorithm that achieves an optimal PAC bound for binary classification. Specifically, our result shows that certain aggregations of one-inclusion graph algorithms are optimal, addressing a variant of a classic question posed by Warmuth.

We further instantiate our framework in three settings where uniform convergence is provably suboptimal. For multiclass classification, we prove an optimal risk bound that scales with the one-inclusion hypergraph density of the class, addressing the suboptimality of the analysis of Daniely and Shalev-Shwartz. For partial hypothesis classification, we determine the optimal sample complexity bound, resolving a question posed by Alon, Hanneke, Holzman, and Moran. For realizable bounded regression with absolute loss, we derive an optimal risk bound that relies on a modified version of the scale-sensitive dimension, refining the results of Bartlett and Long. Our rates surpass standard uniform convergence-based results due to the smaller complexity measure in our risk bound.
</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">COLT</abbr>
    
  
  </div>

  <div id="One-Inclusion-Graph-Not-Optimal" class="col-sm-8">
    
      <div class="title">The One-Inclusion Graph Algorithm is not Always Optimal</div>
      <div class="author">
        
          
            
              
                <em>Ishaq Aden-Ali</em>,
              
            
          
        
          
            
              
                
                  <a href="https://yeshwanth94.github.io/" target="_blank">Yeshwanth Cherapanamjeri</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://ashettyv.github.io/" target="_blank">Abhishek Shetty</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://sites.google.com/view/nikitazhivotovskiy/" target="_blank">Nikita Zhivotovskiy</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Learning Theory,</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2212.09270" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth achieves an optimal in-expectation risk bound in the standard PAC classification setup. In one of the first COLT open problems, Warmuth conjectured that this prediction strategy <i>always</i> implies an optimal high probability bound on the risk, and hence is also an optimal PAC algorithm. We refute this conjecture in the strongest sense: for any practically interesting Vapnik-Chervonenkis class, we provide an in-expectation optimal one-inclusion graph algorithm whose high probability risk bound cannot go beyond that implied by Markov’s inequality. Our construction of these poorly performing one-inclusion graph algorithms uses Varshamov-Tenengolts error correcting codes.

Our negative result has several implications. First, it shows that the same poor high-probability performance is inherited by several recent prediction strategies based on generalizations of the one-inclusion graph algorithm. Second, our analysis shows yet another statistical problem that enjoys an estimator that is provably optimal in expectation via a leave-one-out argument, but fails in the high-probability regime. This discrepancy occurs despite the boundedness of the binary loss for which arguments based on concentration inequalities often provide sharp high probability risk bounds.
</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ALT</abbr>
    
  
  </div>

  <div id="Private-Gaussians" class="col-sm-8">
    
      <div class="title">On the Sample Complexity of Privately Learning Unbounded High-Dimensional Gaussians</div>
      <div class="author">
        
          
            
              
                <em>Ishaq Aden-Ali</em>,
              
            
          
        
          
            
              
                
                  <a href="https://www.cas.mcmaster.ca/ashtiani/" target="_blank">Hassan Ashtiani</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="http://www.gautamkamath.com" target="_blank">Gautam Kamath</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Algorithmic Learning Theory,</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2010.09929" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We provide sample complexity upper bounds for agnostically learning multivariate Gaussians under the constraint of approximate differential privacy. These are the first finite sample upper bounds for general Gaussians which do not impose restrictions on the parameters of the distribution. Our bounds are near-optimal in the case when the covariance is known to be the identity, and conjectured to be near-optimal in the general case. From a technical standpoint, we provide analytic tools for arguing the existence of global “locally small” covers from local covers of the space. These are exploited using modifications of recent techniques for differentially private hypothesis selection. Our techniques may prove useful for privately learning other distribution classes which do not possess a finite cover.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
    <div class="social">
      <span class="contact-icon text-center">
  <a href="mailto:%61%64%65%6E%61%6C%69@%62%65%72%6B%65%6C%65%79.%65%64%75"><i class="fas fa-envelope"></i></a>
  
  
  
  
  
  
  <a href="https://twitter.com/AdenIshaq" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
  
</span>

      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2024 Ishaq  Aden-Ali.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
