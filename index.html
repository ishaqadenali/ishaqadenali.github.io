<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ishaq  Aden-Ali</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://raw.githubusercontent.com/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->




    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Ishaq</span>   Aden-Ali
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic3.jpeg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I’m a final year Ph.D. student at UC Berkeley where I am very fortunate to be co-advised by <a href="https://www.stat.berkeley.edu/~bartlett/" target="\_blank">Peter Bartlett</a> and <a href="https://people.eecs.berkeley.edu/~minilek/" target="\_blank">Jelani Nelson</a>. My name is pronounced <a href="https://en.wikipedia.org/wiki/Ishak_(name)" target="\_blank">Is-hak</a>.</p>

<p><strong> Research Interests: </strong></p>

<!-- I am mainly interested in studying problems at the intersection of theoretical computer science, machine learning and statistics. My current research focuses on questions related to [statistical learning theory](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning){:target="\_blank"} and  [distribution learning](https://en.wikipedia.org/wiki/Distribution_learning_theory){:target="\_blank"}. In particular, I'm interested in understanding which problems can (or cannot!) be provably solved by algorithms that are both statistically and computationally efficient. -->

<!--I am also very interested in private data analysis (via [differential privacy](https://en.wikipedia.org/wiki/Differential_privacy){:target="\_blank"}) and mathematical optimization. -->

<!-- 14/03/23 I'm mainly interested in studying problems at the intersection of theoretical computer science, machine learning, and statistics. I'm currently thinking about learning and estimation problems motivated by practical considerations such as data privacy, model misspecification, and memory limitations. In particular, I'm interested in understanding which of these problems can (or cannot!) be provably solved by algorithms that are both statistically and computationally efficient. -->

<!-- 09/02/2025 I'm generally interested in learning theory, statistics, and theoretical computer science.
A particular interest of mine is to understand how we can borrow mathematical tools developed in one of these research areas to solve problems in another. -->
<!-- 17/12/2025 I am generally interested in statistical learning theory, high dimensional probability, and theoretical computer science.
I am particularly interested in understanding how we can borrow mathematical tools developed in one of these research areas to solve problems in another. -->

<!-- For most of my PhD, I have been interested in statistical learning theory, high dimensional probability, and theoretical computer science.  -->

<!-- I study the foundations of machine learning --- the main paradigm behind modern AI. -->
<p>My research focuses on the foundations of <em> machine learning</em>, the core paradigm behind modern AI.
I currently work on large language models.
<!-- Previously, my work addressed foundational questions in statistical learning theory, high-dimensional probability, and theoretical computer science.-->
Previously, I studied fundamental questions in statistical learning theory, high-dimensional probability, and theoretical computer science.
<!-- My previous work on statistical learning theory, high dimensional probability, and theoretical computer science. --></p>

<!-- Learning Theory: Many classic results in statistical learning theory are based on uniform convergence phenomena that allows one to justify the use of empirical risk minimization procedure. Unfortunately many modern learning settings do not enjoy uniform convergence and cannot enjoy empirical use minimization. Need new algorithmic ideas to do provable learning.
-->

<!-- 
Put your address / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/){:target="\_blank"} and [Academicons](https://jpswalsh.github.io/academicons/){:target="\_blank"}, like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

    </div>

    
      <!-- <div class="news">
  <h2>News</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Nov 9, 2022</th>
          <td>
            
              <a href="https://arxiv.org/abs/2211.03917" target="blank">New paper</a> with Yanjun Han, Jelani Nelson, and Huacheng Yu where we study approximate counting datastructures.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 13, 2021</th>
          <td>
            
              I successfully defended my M.Sc. thesis. I will be joining the EECS department at UC Berkeley as a Ph.D. student.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 7, 2021</th>
          <td>
            
              <a href="https://arxiv.org/abs/2106.02162" target="blank">New paper</a> on privately learning mixtures of axis-aligned Gaussians with fantastic co-authors <a href="https://www.cas.mcmaster.ca/ashtiani/" target="blank">Hassan</a> and <a href="https://cvliaw.github.io/" target="blank">Chris</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 21, 2020</th>
          <td>
            
              <a href="https://arxiv.org/abs/2010.09929" target="blank">Our paper</a> on privately learning high-dimensional Gaussians with <a href="https://www.cas.mcmaster.ca/ashtiani/" target="blank">Hassan</a> and <a href="http://www.gautamkamath.com/" target="blank">Gautam</a> was accepted to <a href="http://algorithmiclearningtheory.org/alt2021/" target="blank">ALT 2021</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 26, 2020</th>
          <td>
            
              I (virtually) presented our <a href="https://arxiv.org/abs/1912.02765" target="blank">paper</a> at AISTATS 2020.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>
 -->
 <!-- Commented out the above to remove news on webiste -->
    

    
      <div class="publications">
  <h2>Selected publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Preprint</abbr>
    
  
  </div>

  <div id="Subliminal_llms" class="col-sm-8">
    
      <div class="title">Subliminal Effects in Your Data: A General Mechanism via Log-Linearity</div>
      <div class="author">
        
          
            
              
                <em>Ishaq Aden-Ali</em>,
              
            
          
        
          
            
              
                
                  <a href="https://noahgol.github.io/" target="_blank">Noah Golowich</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://aliu42.github.io/" target="_blank">Allen Liu</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://ashettyv.github.io/" target="_blank">Abhishek Shetty</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://people.csail.mit.edu/moitra/" target="_blank">Ankur Moitra</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://people.eecs.berkeley.edu/~nika/" target="_blank">Nika Haghtalab</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Preprint,</em>
      
      
        2026
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2602.04863" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/ishaqadenali/logit-linear-selection" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model’s properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.

  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Preprint</abbr>
    
  
  </div>

  <div id="Tensors" class="col-sm-8">
    
      <div class="title">On the Injective Norm of Sums of Random Tensors and the Moments of Gaussian Chaoses</div>
      <div class="author">
        
          
            
              <em>Ishaq Aden-Ali</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>Preprint,</em>
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2503.10580" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We prove an upper bound on the expected \( \ell_p \)&nbsp;injective norm of sums of subgaussian random tensors.
Our proof is simple and does not rely on any explicit geometric or chaining arguments.
Instead, it follows from a simple application of the <i>PAC-Bayesian lemma</i>, a tool that has proven effective at controlling the suprema of certain “smooth” empirical processes in recent years.
Our bound strictly improves a very recent result of Bandeira, Gopi, Jiang, Lucca, and Rothvoss.
In the Euclidean case (\(p=2\)), our bound sharpens a result of Latała that was central to proving his estimates on the moments of Gaussian chaoses.
As a consequence, we obtain an elementary proof of this fundamental result.
</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">COLT</abbr>
    
  
  </div>

  <div id="Majority-of-3" class="col-sm-8">
    
      <div class="title">Majority-of-Three: The Simplest Optimal Learner?</div>
      <div class="author">
        
          
            
              
                <em>Ishaq Aden-Ali</em>,
              
            
          
        
          
            
              
                
                  Mikael Møller Høgsgaard,
                
              
            
          
        
          
            
              
                
                  <a href="https://cs.au.dk/~larsen/" target="_blank">Kasper Green Larsen</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://sites.google.com/view/nikitazhivotovskiy/" target="_blank">Nikita Zhivotovskiy</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Conference on Learning Theory,</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2403.08831" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Developing an optimal PAC learning algorithm in the realizable setting, where empirical risk minimization (ERM) is suboptimal, was a major open problem in learning theory for decades. The problem was finally resolved by Hanneke a few years ago. Unfortunately, Hanneke’s algorithm is quite complex as it returns the majority vote of many ERM classifiers that are trained on carefully selected subsets of the data. It is thus a natural goal to determine the simplest algorithm that is optimal. In this work we study the arguably simplest algorithm that could be optimal: returning the majority vote of three ERM classifiers. We show that this algorithm achieves the optimal in-expectation bound on its error which is provably unattainable by a single ERM classifier. Furthermore, we prove a near-optimal high-probability bound on this algorithm’s error. We conjecture that a better analysis will prove that this algorithm is in fact optimal in the high-probability regime.
</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">FOCS</abbr>
    
  
  </div>

  <div id="One-Inclusion-Graph-Optimal" class="col-sm-8">
    
      <div class="title">Optimal PAC Bounds Without Uniform Convergence</div>
      <div class="author">
        
          
            
              
                <em>Ishaq Aden-Ali</em>,
              
            
          
        
          
            
              
                
                  <a href="https://yeshwanth94.github.io/" target="_blank">Yeshwanth Cherapanamjeri</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://ashettyv.github.io/" target="_blank">Abhishek Shetty</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://sites.google.com/view/nikitazhivotovskiy/" target="_blank">Nikita Zhivotovskiy</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Symposium on Foundations of Computer Science,</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2304.09167" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In statistical learning theory, determining the sample complexity of realizable binary classification for VC classes was a long-standing open problem. The results of Simon and Hanneke established sharp upper bounds in this setting. However, the reliance of their argument on the uniform convergence principle limits its applicability to more general learning settings such as multiclass classification. In this paper, we address this issue by providing optimal high probability risk bounds through a framework that surpasses the limitations of uniform convergence arguments.
Our framework converts the leave-one-out error of permutation invariant predictors into high probability risk bounds. As an application, by adapting the one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth, we propose an algorithm that achieves an optimal PAC bound for binary classification. Specifically, our result shows that certain aggregations of one-inclusion graph algorithms are optimal, addressing a variant of a classic question posed by Warmuth.

We further instantiate our framework in three settings where uniform convergence is provably suboptimal. For multiclass classification, we prove an optimal risk bound that scales with the one-inclusion hypergraph density of the class, addressing the suboptimality of the analysis of Daniely and Shalev-Shwartz. For partial hypothesis classification, we determine the optimal sample complexity bound, resolving a question posed by Alon, Hanneke, Holzman, and Moran. For realizable bounded regression with absolute loss, we derive an optimal risk bound that relies on a modified version of the scale-sensitive dimension, refining the results of Bartlett and Long. Our rates surpass standard uniform convergence-based results due to the smaller complexity measure in our risk bound.
</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
    <div class="social">
      <span class="contact-icon text-center">
  <a href="mailto:%61%64%65%6E%61%6C%69@%62%65%72%6B%65%6C%65%79.%65%64%75"><i class="fas fa-envelope"></i></a>
  
  
  
  
  
  
  <a href="https://twitter.com/AdenIshaq" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
  
</span>

      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2026 Ishaq  Aden-Ali.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    Last updated: February 04, 2026.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
